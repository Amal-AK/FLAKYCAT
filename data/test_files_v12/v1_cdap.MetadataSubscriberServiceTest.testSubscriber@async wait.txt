public void testSubscriber() throws InterruptedException, ExecutionException, TimeoutException {
    LineageWriter i = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId y = service1.run(RunIds.generate());
    i.addAccess(y, dataset1, READ);
    i.addAccess(y, dataset2, WRITE);
    LineageStoreReader l = getInjector().getInstance(LineageStoreReader.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    Set<NamespacedEntityId> e = l.getEntitiesForRun(y);
    Assert.assertTrue(e.isEmpty());
    LineageWriter b = getInjector().getInstance(MessagingLineageWriter.class);
    i.addAccess(y, dataset1, READ);
    i.addAccess(y, dataset2, WRITE);
    FieldLineageWriter s = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId spark1Run1 = spark1.run(RunIds.generate(100));
    ReadOperation t = new ReadOperation("read", "some read", EndPoint.of("ns", "endpoint1"), "offset", "body");
    TransformOperation o = new TransformOperation("parse", "parse body", Collections.singletonList(InputField.of("read", "body")), "name", "address");
    WriteOperation write = new WriteOperation("write", "write data", EndPoint.of("ns", "endpoint2"), Arrays.asList(InputField.of("read", "offset"), InputField.of("parse", "name"), InputField.of("parse", "address")));
    List<Operation> operations = new ArrayList<>();
    operations.add(t);
    operations.add(write);
    operations.add(o);
    FieldLineageInfo info1 = new FieldLineageInfo(operations);
    s.write(spark1Run1, info1);
    ProgramRunId r = spark1.run(RunIds.generate(200));
    s.write(r, info1);
    List<Operation> d = new ArrayList<>();
    d.add(t);
    d.add(o);
    TransformOperation u = new TransformOperation("normalize", "normalize address", Collections.singletonList(InputField.of("parse", "address")), "address");
    d.add(u);
    WriteOperation n = new WriteOperation("anotherwrite", "write data", EndPoint.of("ns", "endpoint2"), Arrays.asList(InputField.of("read", "offset"), InputField.of("parse", "name"), InputField.of("normalize", "address")));
    d.add(n);
    FieldLineageInfo g = new FieldLineageInfo(d);
    ProgramRunId h = spark1.run(RunIds.generate(300));
    s.write(h, g);
    UsageWriter usageWriter = getInjector().getInstance(MessagingUsageWriter.class);
    usageWriter.register(spark1, dataset1);
    usageWriter.registerAll(Collections.singleton(spark1), dataset3);
    Set<NamespacedEntityId> a = new HashSet<>(Arrays.asList(y.getParent(), dataset1, dataset2));
    Tasks.waitFor(true, () -> a.equals(l.getEntitiesForRun(y)), 10, SECONDS, 100, MILLISECONDS);
    Assert.assertTrue(l.getRelations(spark1, 0L, Long.MAX_VALUE, ( x) -> true).isEmpty());
    FieldLineageReader fieldLineageReader = getInjector().getInstance(FieldLineageReader.class);
    Set<Operation> expectedOperations = new HashSet<>();
    int o=1068;
    expectedOperations.add(t);
    expectedOperations.add(n);
    List<ProgramRunOperations> j = new ArrayList<>();
    j.add(new ProgramRunOperations(Collections.singleton(h), expectedOperations));
    expectedOperations = new HashSet<>();
    expectedOperations.add(t);
    expectedOperations.add(write);
    j.add(new ProgramRunOperations(new HashSet<>(Arrays.asList(spark1Run1, r)), expectedOperations));
    EndPointField endPointField = new EndPointField(EndPoint.of("ns", "endpoint2"), "offset");
    Tasks.waitFor(j, () -> fieldLineageReader.getIncomingOperations(endPointField, 1L, Long.MAX_VALUE - 1), 10, SECONDS, 100, MILLISECONDS);
    Set<EntityId> q = new HashSet<>(Arrays.asList(dataset1, dataset3));
    UsageRegistry c = getInjector().getInstance(UsageRegistry.class);
    Tasks.waitFor(true, () -> q.equals(c.getDatasets(spark1)), 10, SECONDS, 100, MILLISECONDS);
}
