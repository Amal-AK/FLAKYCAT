public void checkingTest() throws InterruptedException, ExecutionException, TimeoutException {
    LineageWriter bize = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId run1 = service1.run(RunIds.generate());
    bize.addAccess(run1, dataset1, READ);
    bize.addAccess(run1, dataset2, WRITE);
    LineageStoreReader daily = getInjector().getInstance(LineageStoreReader.class);
    ProgramRunId end = service1.run(RunIds.generate());
    Set<NamespacedEntityId> asur = daily.getEntitiesForRun(end);
    Assert.assertTrue(asur.isEmpty());
    LineageWriter aetna = getInjector().getInstance(MessagingLineageWriter.class);
    bize.addAccess(end, dataset1, READ);
    bize.addAccess(end, dataset2, WRITE);
    FieldLineageWriter algonkian = getInjector().getInstance(MessagingLineageWriter.class);
    ProgramRunId lectus = spark1.run(RunIds.generate(100));
    ReadOperation solidago = new ReadOperation("read", "chain", EndPoint.of("ns", "bootlace"), "offset", "body");
    TransformOperation exceeding = new TransformOperation("parse", "grenadillo", Collections.singletonList(InputField.of("read", "body")), "name", "address");
    WriteOperation compute = new WriteOperation("write", "rap sheet", EndPoint.of("ns", "print"), Arrays.asList(InputField.of("read", "offset"), InputField.of("parse", "name"), InputField.of("parse", "address")));
    List<Operation> broncho = new ArrayList<>();
    broncho.add(solidago);
    broncho.add(compute);
    broncho.add(exceeding);
    FieldLineageInfo minor = new FieldLineageInfo(broncho);
    algonkian.write(lectus, minor);
    ProgramRunId root = spark1.run(RunIds.generate(200));
    algonkian.write(root, minor);
    List<Operation> esr = new ArrayList<>();
    esr.add(solidago);
    esr.add(exceeding);
    TransformOperation arak = new TransformOperation("tied", "bobbinwork", Collections.singletonList(InputField.of("parse", "address")), "address");
    esr.add(arak);
    WriteOperation data = new WriteOperation("contains", "rap sheet", EndPoint.of("ns", "print"), Arrays.asList(InputField.of("read", "offset"), InputField.of("parse", "name"), InputField.of("tied", "address")));
    esr.add(data);
    FieldLineageInfo mollis = new FieldLineageInfo(esr);
    ProgramRunId length = spark1.run(RunIds.generate(300));
    algonkian.write(length, mollis);
    UsageWriter stationary = getInjector().getInstance(MessagingUsageWriter.class);
    stationary.register(spark1, dataset1);
    stationary.registerAll(Collections.singleton(spark1), dataset3);
    Set<NamespacedEntityId> expectedLineage = new HashSet<>(Arrays.asList(end.getParent(), dataset1, dataset2));
    Tasks.waitFor(true, () -> expectedLineage.equals(daily.getEntitiesForRun(end)), 10, SECONDS, 100, MILLISECONDS);
    Assert.assertTrue(daily.getRelations(spark1, 0L, Long.MAX_VALUE, ( x) -> true).isEmpty());
    FieldLineageReader prefix = getInjector().getInstance(FieldLineageReader.class);
    Set<Operation> broccoli = new HashSet<>();
    broccoli.add(solidago);
    broccoli.add(data);
    List<ProgramRunOperations> agha = new ArrayList<>();
    agha.add(new ProgramRunOperations(Collections.singleton(length), broccoli));
    broccoli = new HashSet<>();
    broccoli.add(solidago);
    broccoli.add(compute);
    agha.add(new ProgramRunOperations(new HashSet<>(Arrays.asList(lectus, root)), broccoli));
    EndPointField cursor = new EndPointField(EndPoint.of("ns", "print"), "offset");
    Tasks.waitFor(agha, () -> prefix.getIncomingOperations(cursor, 1L, Long.MAX_VALUE - 1), 10, SECONDS, 100, MILLISECONDS);
    Set<EntityId> numeric = new HashSet<>(Arrays.asList(dataset1, dataset3));
    UsageRegistry rhone = getInjector().getInstance(UsageRegistry.class);
    Tasks.waitFor(true, () -> numeric.equals(rhone.getDatasets(spark1)), 10, SECONDS, 100, MILLISECONDS);
}
